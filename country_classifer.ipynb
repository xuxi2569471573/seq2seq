{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gzip\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import time\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 256\n",
    "N_LAYER = 2\n",
    "N_EPOCHS = 10\n",
    "N_CHARS = 128\n",
    "USE_GPU = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor(tensor):\n",
    "    if USE_GPU:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        tensor = tensor.to(device)\n",
    "    return tensor\n",
    "def name2list(name):\n",
    "    arr = [ord(c) for c in name]\n",
    "    return arr, len(arr)\n",
    "def make_tensors(names, countries):\n",
    "    sequences_and_lengths = [name2list(name) for name in names]\n",
    "    name_sequences = [sl[0] for sl in sequences_and_lengths]\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequences_and_lengths])\n",
    "    countries = countries.long()\n",
    "    # make tensor of name, BatchSize x SeqLen\n",
    "    seq_tensor = torch.zeros(len(name_sequences), seq_lengths.max()).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(name_sequences, seq_lengths), 0):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "    # sort by length to use pack_padded_sequence\n",
    "    seq_lengths, perm_idx = seq_lengths.sort(dim=0, descending=True)\n",
    "    seq_tensor = seq_tensor[perm_idx]\n",
    "    countries = countries[perm_idx]\n",
    "    return  create_tensor(seq_tensor),\\\n",
    "            create_tensor(seq_lengths),\\\n",
    "            create_tensor(countries)\n",
    "            #BatchSize x SeqLen,BatchSize,BatchSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([101, 104])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#name2list('hello'),torch.tensor([104, 101, 108, 108, 111]).sort(dim=0, descending=True),name2list('hello')[0][torch.tensor([1,2])]\n",
    "torch.tensor([104, 101, 108, 108, 111])[torch.tensor([1,0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=True):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_directions = 2 if bidirectional else 1\n",
    "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "        bidirectional=bidirectional)\n",
    "        self.fc = torch.nn.Linear(hidden_size * self.n_directions, output_size)\n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers * self.n_directions,\n",
    "        batch_size, self.hidden_size)\n",
    "        return create_tensor(hidden)\n",
    "    def forward(self, input, seq_lengths):\n",
    "        # input shape : B x S -> S x B\n",
    "        input = input.t()\n",
    "        batch_size = input.size(1)\n",
    "        hidden = self._init_hidden(batch_size)\n",
    "        embedding = self.embedding(input)\n",
    "        # pack them up\n",
    "        gru_input = nn.utils.rnn.pack_padded_sequence(embedding, seq_lengths)\n",
    "        output, hidden = self.gru(gru_input, hidden)\n",
    "        if self.n_directions == 2:\n",
    "            hidden_cat = torch.cat([hidden[-1], hidden[-2]], dim=1)\n",
    "        else:\n",
    "            hidden_cat = hidden[-1]\n",
    "        fc_output = self.fc(hidden_cat)\n",
    "        return fc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameDataset(Dataset):\n",
    "    def __init__(self, is_train_set=True):\n",
    "        filename = 'data/names_train.csv.gz' if is_train_set else 'data/names_test.csv.gz'\n",
    "        with gzip.open(filename, 'rt') as f:\n",
    "            reader = csv.reader(f)\n",
    "            rows = list(reader)\n",
    "        self.names = [row[0] for row in rows]\n",
    "        self.len = len(self.names)\n",
    "        self.countries = [row[1] for row in rows]\n",
    "        self.country_list = list(sorted(set(self.countries)))\n",
    "        self.country_dict = self.getCountryDict()\n",
    "        self.country_num = len(self.country_list)\n",
    "    def __getitem__(self, index):\n",
    "        return self.names[index], self.country_dict[self.countries[index]]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    def getCountryDict(self):\n",
    "        country_dict = dict()\n",
    "        for idx, country_name in enumerate(self.country_list, 0):\n",
    "            country_dict[country_name] = idx\n",
    "        return country_dict\n",
    "    def idx2country(self, index): \n",
    "            return self.country_list[index]\n",
    "    def getCountriesNum(self):\n",
    "        return self.country_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = NameDataset(is_train_set=False)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "trainset = NameDataset(is_train_set=True)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "N_COUNTRY = trainset.getCountriesNum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arabic': 0,\n",
       " 'Chinese': 1,\n",
       " 'Czech': 2,\n",
       " 'Dutch': 3,\n",
       " 'English': 4,\n",
       " 'French': 5,\n",
       " 'German': 6,\n",
       " 'Greek': 7,\n",
       " 'Irish': 8,\n",
       " 'Italian': 9,\n",
       " 'Japanese': 10,\n",
       " 'Korean': 11,\n",
       " 'Polish': 12,\n",
       " 'Portuguese': 13,\n",
       " 'Russian': 14,\n",
       " 'Scottish': 15,\n",
       " 'Spanish': 16,\n",
       " 'Vietnamese': 17}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.getCountryDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10 epochs...\n",
      "[0m 3s] Epoch 1 [2560/13374] loss=0.008864981774240731\n",
      "[0m 5s] Epoch 1 [5120/13374] loss=0.007563631841912865\n",
      "[0m 7s] Epoch 1 [7680/13374] loss=0.006881272746250033\n",
      "[0m 9s] Epoch 1 [10240/13374] loss=0.0064178886939771475\n",
      "[0m 11s] Epoch 1 [12800/13374] loss=0.0060393330920487645\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 4514/6700 67.37%\n",
      "[0m 14s] Epoch 2 [2560/13374] loss=0.00406025790143758\n",
      "[0m 16s] Epoch 2 [5120/13374] loss=0.004083089565392583\n",
      "[0m 18s] Epoch 2 [7680/13374] loss=0.004014090821146965\n",
      "[0m 20s] Epoch 2 [10240/13374] loss=0.003938831842970103\n",
      "[0m 22s] Epoch 2 [12800/13374] loss=0.003855418264865875\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 4947/6700 73.84%\n",
      "[0m 25s] Epoch 3 [2560/13374] loss=0.003352350159548223\n",
      "[0m 27s] Epoch 3 [5120/13374] loss=0.0032870883238501846\n",
      "[0m 29s] Epoch 3 [7680/13374] loss=0.0031968329567462206\n",
      "[0m 31s] Epoch 3 [10240/13374] loss=0.003120268328348175\n",
      "[0m 33s] Epoch 3 [12800/13374] loss=0.0030941906291991473\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5211/6700 77.78%\n",
      "[0m 36s] Epoch 4 [2560/13374] loss=0.002832330879755318\n",
      "[0m 38s] Epoch 4 [5120/13374] loss=0.002762526087462902\n",
      "[0m 40s] Epoch 4 [7680/13374] loss=0.002686383784748614\n",
      "[0m 42s] Epoch 4 [10240/13374] loss=0.0026370942243374882\n",
      "[0m 44s] Epoch 4 [12800/13374] loss=0.002614578646607697\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5355/6700 79.93%\n",
      "[0m 47s] Epoch 5 [2560/13374] loss=0.0022621705546043815\n",
      "[0m 49s] Epoch 5 [5120/13374] loss=0.002277036028681323\n",
      "[0m 51s] Epoch 5 [7680/13374] loss=0.0022897163600039978\n",
      "[0m 54s] Epoch 5 [10240/13374] loss=0.002280214286292903\n",
      "[0m 56s] Epoch 5 [12800/13374] loss=0.002291507471818477\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5425/6700 80.97%\n",
      "[0m 59s] Epoch 6 [2560/13374] loss=0.0020436003105714917\n",
      "[1m 1s] Epoch 6 [5120/13374] loss=0.002068200526991859\n",
      "[1m 3s] Epoch 6 [7680/13374] loss=0.0020556314149871467\n",
      "[1m 5s] Epoch 6 [10240/13374] loss=0.0020708049705717713\n",
      "[1m 7s] Epoch 6 [12800/13374] loss=0.0020695231156423687\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5481/6700 81.81%\n",
      "[1m 11s] Epoch 7 [2560/13374] loss=0.0018411527620628477\n",
      "[1m 13s] Epoch 7 [5120/13374] loss=0.001827192079508677\n",
      "[1m 15s] Epoch 7 [7680/13374] loss=0.0018566392866584162\n",
      "[1m 17s] Epoch 7 [10240/13374] loss=0.001840762948268093\n",
      "[1m 19s] Epoch 7 [12800/13374] loss=0.0018369318707846104\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5506/6700 82.18%\n",
      "[1m 22s] Epoch 8 [2560/13374] loss=0.0016178831341676415\n",
      "[1m 24s] Epoch 8 [5120/13374] loss=0.0015873252181336285\n",
      "[1m 26s] Epoch 8 [7680/13374] loss=0.0016682726059419413\n",
      "[1m 28s] Epoch 8 [10240/13374] loss=0.0016684453381458297\n",
      "[1m 30s] Epoch 8 [12800/13374] loss=0.001668033378664404\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5575/6700 83.21%\n",
      "[1m 33s] Epoch 9 [2560/13374] loss=0.001596661622170359\n",
      "[1m 35s] Epoch 9 [5120/13374] loss=0.0015590676979627461\n",
      "[1m 37s] Epoch 9 [7680/13374] loss=0.0015421989684303603\n",
      "[1m 39s] Epoch 9 [10240/13374] loss=0.0015288680471712724\n",
      "[1m 41s] Epoch 9 [12800/13374] loss=0.0015235671144910156\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5585/6700 83.36%\n",
      "[1m 45s] Epoch 10 [2560/13374] loss=0.0013304444262757897\n",
      "[1m 47s] Epoch 10 [5120/13374] loss=0.0013543971523176878\n",
      "[1m 49s] Epoch 10 [7680/13374] loss=0.0013706715932736795\n",
      "[1m 51s] Epoch 10 [10240/13374] loss=0.0013487506148521788\n",
      "[1m 52s] Epoch 10 [12800/13374] loss=0.0013581316790077834\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5613/6700 83.78%\n"
     ]
    }
   ],
   "source": [
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "def trainModel():\n",
    "    total_loss = 0\n",
    "    for i, (names, countries) in enumerate(trainloader, 1):\n",
    "        inputs, seq_lengths, target = make_tensors(names, countries)\n",
    "        output = classifier(inputs, seq_lengths)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print(f'[{time_since(start)}] Epoch {epoch} ', end='')\n",
    "            print(f'[{i * len(inputs)}/{len(trainset)}] ', end='')\n",
    "            print(f'loss={total_loss / (i * len(inputs))}')\n",
    "    return total_loss\n",
    "def testModel():\n",
    "    correct = 0\n",
    "    total = len(testset)\n",
    "    print(\"evaluating trained model ...\")\n",
    "    with torch.no_grad():\n",
    "        for i, (names, countries) in enumerate(testloader, 1):\n",
    "            inputs, seq_lengths, target = make_tensors(names, countries)\n",
    "            output = classifier(inputs, seq_lengths)\n",
    "            pred = output.max(dim=1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        percent = '%.2f' % (100 * correct / total)\n",
    "        print(f'Test set: Accuracy {correct}/{total} {percent}%')\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRY, N_LAYER)\n",
    "if USE_GPU:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    classifier.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "start = time.time()\n",
    "print(\"Training for %d epochs...\" % N_EPOCHS)\n",
    "acc_list = []\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "# Train cycle\n",
    "    trainModel()\n",
    "    acc = testModel()\n",
    "    acc_list.append(acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = [1, 2, 3]\n",
    "# file = open('node.txt', 'a')\n",
    "# #mid = str(node)\n",
    "# # mid = str(node).replace('[', '').replace(']', '')\n",
    "# # # 删除单引号并用字符空格代替逗号\n",
    "# # mid = mid.replace(\"'\", '').replace(',', '') + '\\n'\n",
    "# file.write(node)\n",
    "# file.close()\n",
    "\n",
    "with open('node.txt', 'a') as f:\n",
    "    f.write(str(node).replace('[', '').replace(']', '').replace(',', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "node_pair1 = np.loadtxt('./node.txt', dtype=bytes).astype(int)\n",
    "node_pair = list(node_pair1)\n",
    "print(node_pair)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "985a0f387c27ff784145167e4daf92db03db1d9b0a87f6ca13a539e928ea05de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
